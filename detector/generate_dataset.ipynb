{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detector: Dataset Creation\n",
    "\n",
    "This notebook generates the datasets for creating an object detector using TF models. This object detector will detect 4 seperate classes:\n",
    "\n",
    " - 1 - Clouds\n",
    " - 2 - the Sun\n",
    " - 3 - Houses\n",
    " - 4 - Trees\n",
    "\n",
    "Sources:\n",
    "- [1] https://www.oreilly.com/ideas/object-detection-with-tensorflow\n",
    "- [2] https://github.com/tzutalin/labelImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T22:16:30.508902Z",
     "start_time": "2018-02-03T22:16:30.475429Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Image sizes\n",
    "MAX_WIDTH = 1280\n",
    "MAX_HEIGHT = 300\n",
    "\n",
    "# Training params\n",
    "TRAIN_TEST_SPLIT = 0.9\n",
    "\n",
    "# Class dictionary\n",
    "CLASS_NAMES = {1:'cloud', 2:'sun', 3:'house', 4:'tree'}\n",
    "\n",
    "# Define paths to sub-folders\n",
    "root_dir = Path.cwd()\n",
    "images_path = root_dir / 'images'\n",
    "labels_path = root_dir / 'labels'\n",
    "train_path = root_dir / 'models'\n",
    "data_path = root_dir / 'data'\n",
    "\n",
    "# Output filenames paths\n",
    "train_tfrecord_path = data_path / 'train.record'\n",
    "test_tfrecord_path = data_path / 'test.record'\n",
    "labels_csv_path = data_path / 'labels.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label images\n",
    "\n",
    "Label the images using LabelImg [2]. This creates an xml file for each image with the bounding boxes and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T22:16:33.822355Z",
     "start_time": "2018-02-03T22:16:33.780820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted xmls to csv file\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Convert the XMLs into a single CSV file\n",
    "xml_list = []\n",
    "for xml_path in list(labels_path.glob('*.xml')):\n",
    "    tree = ET.parse(str(xml_path))\n",
    "    root = tree.getroot()\n",
    "    for member in root.findall('object'):\n",
    "        # Unpack each object (BB) from the xml\n",
    "        value = (root.find('filename').text,\n",
    "                 int(root.find('size')[0].text),\n",
    "                 int(root.find('size')[1].text),\n",
    "                 member[0].text,\n",
    "                 int(member[4][0].text),\n",
    "                 int(member[4][1].text),\n",
    "                 int(member[4][2].text),\n",
    "                 int(member[4][3].text))\n",
    "        xml_list.append(value)\n",
    "# Create pandas dataframe from the labels in the XML\n",
    "column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "xml_df.to_csv(str(labels_csv_path), index=None)\n",
    "print('Converted xmls to csv file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TFRecords\n",
    "\n",
    "Convert to TFRecords so we can train better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T22:17:07.009500Z",
     "start_time": "2018-02-03T22:17:06.895407Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "\n",
    "def to_tfrecords(image_paths, labels_path, tfrecord_path):\n",
    "    if tfrecord_path.exists():\n",
    "        print('TFRecord already created, delete it before making a new one')\n",
    "        return\n",
    "    writer = tf.python_io.TFRecordWriter(str(tfrecord_path))\n",
    "    # Read labels from csv\n",
    "    label_df = pd.read_csv(str(labels_path))\n",
    "    gb = label_df.groupby('filename')\n",
    "    # Convert each image to a tfrecords example then write\n",
    "    for image_path in image_paths:\n",
    "        try:\n",
    "            group = gb.get_group(image_path.name)\n",
    "        except KeyError:\n",
    "            print('Could not find labels for %s' % image_path.name)\n",
    "            continue\n",
    "        # Write each serialized example to writer\n",
    "        writer.write(_create_tf_example(image_path, group).SerializeToString())\n",
    "    writer.close()\n",
    "    print('TFRecord created at %s' % str(tfrecord_path))\n",
    "\n",
    "def _create_tf_example(image_path, groups):\n",
    "    # Read image and encode it\n",
    "    with tf.gfile.GFile(str(image_path), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "    # Feature defines each discrete entry in the tfrecords file\n",
    "    filename = image_path.name.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "    print('groups: ', groups)\n",
    "    for index, row in groups.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes.append(row['class'])\n",
    "        classes_text.append(CLASS_NAMES[row['class']].encode('utf8'))\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T22:17:21.777665Z",
     "start_time": "2018-02-03T22:17:21.622114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 images total, split into 18 train and 2 test\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "50   18.jpg   1280     853      2   563   357   603   395\n",
      "51   18.jpg   1280     853      4   227   320   309   414\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "92   12.jpg    275     183      3   140   122   153   142\n",
      "93   12.jpg    275     183      3    33   115    53   136\n",
      "94   12.jpg    275     183      1    93    26   139    41\n",
      "95   12.jpg    275     183      1   213    14   272    27\n",
      "96   12.jpg    275     183      4   190   153   215   163\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "10   10.jpg   1000     751      1   135    85   319   142\n",
      "11   10.jpg   1000     751      1   547    63   656   107\n",
      "12   10.jpg   1000     751      1   683   108   793   147\n",
      "13   10.jpg   1000     751      1   708     1   944    76\n",
      "14   10.jpg   1000     751      1   424   147   546   182\n",
      "15   10.jpg   1000     751      3   763   307   857   445\n",
      "16   10.jpg   1000     751      4   743   616   806   679\n",
      "17   10.jpg   1000     751      4   241   652   317   723\n",
      "18   10.jpg   1000     751      3   420   303   518   475\n",
      "19   10.jpg   1000     751      3   233   453   323   578\n",
      "20   10.jpg   1000     751      3   614   441   700   572\n",
      "21   10.jpg   1000     751      3   201   292   286   363\n",
      "22   10.jpg   1000     751      4   148   410   216   468\n",
      "groups:    filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "0    8.jpg   1200     900      3   375   523   606   749\n",
      "1    8.jpg   1200     900      3   704   456   876   612\n",
      "2    8.jpg   1200     900      3   607   111   710   634\n",
      "3    8.jpg   1200     900      4   483   447   540   485\n",
      "4    8.jpg   1200     900      4   289   497   368   585\n",
      "5    8.jpg   1200     900      1   632    27   760    67\n",
      "6    8.jpg   1200     900      1   369   176   531   237\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "28   11.jpg    750     422      1   313    61   402    97\n",
      "29   11.jpg    750     422      1   188    88   294   134\n",
      "30   11.jpg    750     422      1   393    83   535   116\n",
      "31   11.jpg    750     422      4   519   278   541   311\n",
      "32   11.jpg    750     422      4   257   372   285   422\n",
      "33   11.jpg    750     422      4   654   213   690   267\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "68    9.jpg    900     600      3   291   221   364   247\n",
      "69    9.jpg    900     600      3   582   209   664   262\n",
      "70    9.jpg    900     600      4   705   240   743   280\n",
      "71    9.jpg    900     600      4   549   402   764   578\n",
      "72    9.jpg    900     600      4   584   162   622   190\n",
      "73    9.jpg    900     600      1   482    68   576   112\n",
      "74    9.jpg    900     600      1   795    48   900    93\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "52   17.jpg   1600    1067      1   116   271   346   373\n",
      "53   17.jpg   1600    1067      2  1511   331  1600   452\n",
      "54   17.jpg   1600    1067      3  1165   575  1351   733\n",
      "55   17.jpg   1600    1067      3  1415   463  1462   584\n",
      "56   17.jpg   1600    1067      4   757   865   874  1015\n",
      "57   17.jpg   1600    1067      4   373   892   569   991\n",
      "58   17.jpg   1600    1067      3   258   504   384   565\n",
      "59   17.jpg   1600    1067      1   462   188   619   299\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "34    6.jpg   1282     722      1   474   259   713   496\n",
      "35    6.jpg   1282     722      4   311   518   472   722\n",
      "36    6.jpg   1282     722      1   836   116   973   222\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "43   19.jpg    275     183      4   126   144   156   175\n",
      "44   19.jpg    275     183      3   227    92   249   120\n",
      "45   19.jpg    275     183      1    80    30   116    52\n",
      "46   19.jpg    275     183      1    99     4   130    28\n",
      "47   19.jpg    275     183      1   218    40   251    60\n",
      "48   19.jpg    275     183      3    45    73    69    93\n",
      "49   19.jpg    275     183      4   115   103   132   118\n",
      "groups:      filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "97     2.jpg    960     540      1   454   226   644   284\n",
      "98     2.jpg    960     540      1   789    12   911    58\n",
      "99     2.jpg    960     540      1   640   153   810   209\n",
      "100    2.jpg    960     540      1   210   266   348   326\n",
      "101    2.jpg    960     540      1   186    39   349   148\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "40   20.jpg    259     194      2   133    29   151    43\n",
      "41   20.jpg    259     194      4    39    25    50    52\n",
      "42   20.jpg    259     194      4   209    34   230    53\n",
      "groups:    filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "7   15.jpg    275     183      2    97    45   122    66\n",
      "8   15.jpg    275     183      3   136    97   203   124\n",
      "9   15.jpg    275     183      3   224   111   266   168\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "85    3.jpg    276     183      1    50    20    80    37\n",
      "86    3.jpg    276     183      1   122   102   153   114\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "37    5.jpg    852     480      1   240    20   420   195\n",
      "38    5.jpg    852     480      1   560    50   655   169\n",
      "39    5.jpg    852     480      1   227   246   429   314\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "75   14.jpg   3640    2153      4  2780  1333  2996  1895\n",
      "76   14.jpg   3640    2153      3  1255   851  2402  1645\n",
      "77   14.jpg   3640    2153      4  1630   554  1896   776\n",
      "78   14.jpg   3640    2153      1  1834   145  2184   264\n",
      "79   14.jpg   3640    2153      3   190   948   777  1533\n",
      "80   14.jpg   3640    2153      4  2677   676  2830   883\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "60   16.jpg   1000     550      2     1    57    67   162\n",
      "61   16.jpg   1000     550      3   689   110   719   189\n",
      "62   16.jpg   1000     550      3   146   254   303   462\n",
      "63   16.jpg   1000     550      3   823   164   907   194\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "87   13.jpg   2698    2706      3   247   853  1182  1823\n",
      "88   13.jpg   2698    2706      4  1152  1371  1360  1728\n",
      "89   13.jpg   2698    2706      4  1379  1299  1574  1623\n",
      "90   13.jpg   2698    2706      3  2244  1201  2698  1591\n",
      "91   13.jpg   2698    2706      4   385  2315   850  2696\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "64    4.jpg    300     168      1   113    33   160    72\n",
      "65    4.jpg    300     168      1   254    25   298    44\n",
      "66    4.jpg    300     168      1   190    48   223    68\n",
      "67    4.jpg    300     168      1   270    53   300    76\n",
      "TFRecord created at /home/ook/repos/iris_challenge/detector/data/train.record\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "23    1.jpg    275     183      1    49    21   147    58\n",
      "24    1.jpg    275     183      1   174    68   202    86\n",
      "25    1.jpg    275     183      1   209    46   236    61\n",
      "26    1.jpg    275     183      1     9    85    73   104\n",
      "27    1.jpg    275     183      1    90    92   130   105\n",
      "groups:     filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "81    7.jpg    279     180      1    43    80    96   101\n",
      "82    7.jpg    279     180      1   227    58   260    75\n",
      "83    7.jpg    279     180      1    70    35   115    72\n",
      "84    7.jpg    279     180      1   155    91   192   101\n",
      "TFRecord created at /home/ook/repos/iris_challenge/detector/data/test.record\n"
     ]
    }
   ],
   "source": [
    "# Split data into test and train\n",
    "image_paths = list(images_path.glob('*.jpg'))\n",
    "num_images = len(image_paths)\n",
    "num_train = int(TRAIN_TEST_SPLIT * num_images)\n",
    "train_index = np.random.choice(num_images, size=num_train, replace=False)\n",
    "test_index = np.setdiff1d(list(range(num_images)), train_index)\n",
    "train_image_paths = [image_paths[i] for i in train_index]\n",
    "test_image_paths = [image_paths[i] for i in test_index]\n",
    "print('There are %d images total, split into %s train and %s test' % (num_images,\n",
    "                                                                      len(train_image_paths),\n",
    "                                                                      len(test_image_paths)))\n",
    "# Convert list of train and test images into a tfrecord\n",
    "to_tfrecords(train_image_paths, labels_csv_path, train_tfrecord_path)\n",
    "to_tfrecords(test_image_paths, labels_csv_path, test_tfrecord_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vectorize]",
   "language": "python",
   "name": "conda-env-vectorize-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
